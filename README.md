# Online Language Modelling Dataset Pipeline

## Setup
1. Get a machine with lots of CPUs and memory. We use an n1-standard-96 Ubuntu 20.04 LTS machine on GCP. Add enough disk space too; at least 1TB if you want to pull a reasonable about of data.
2. Install cargo (rust package manager) with `curl https://sh.rustup.rs -sSf | sh`. Then install Ungoliant with `cargo install ungoliant`. You may need to install gcc and cmake first.
3. Set up a Python 3.8 environment, and run `pip install -r requirements.txt`
4. Run `bash setup_ungoliant_bigscience_pipeline.sh`. This will set up the BigScience filtering code which deduplicates the corpus and filters documents if they have too much pornographic content. It will also download NLP models which are used by both Ungoliant and BigScience to do things such as filter the corpus for English-only pages.
5. Run `huggingface-cli login` (should have been installed in the requirements.txt) and then paste a token from your account at [https://huggingface.co](https://huggingface.co). This is necessary because the pipeline will push the finalized datasets to your account.


## Usage
1. To get a cleaned Common Crawl corpus using the Ungoliant and BigScience scripts, run `bash ungoliant_bigscience_pipeline.sh <crawl> <num_proc> <offset> <huggingface_username>`. Replace the arguments with your desired settings, for example: `CC-MAIN-2022-33 96 78500 Tristan`. The `<offset>` argument is how many Common Crawl segments to skip. There are 80000 segments in total, so an `<offset>` of 78500 means that 1500 segments will be downloaded, resulting in approximately a 30GB English text dataset. After running this command, you should see datasets on the Hugging Face hub under your account that have names like this: `Tristan/ungoliant-for-olm-raw-CC-MAIN-2022-33` and `Tristan/ungoliant-for-olm-clean-CC-MAIN-2022-33`. The raw version is the dataset right after the Ungoliant pipeline compeletes, with articles from Wikipedia URLs removed (we pull Wikipedia in the next step). The clean version is the raw version with deduplication and pornographic language filtering from the BigScience scripts. The whole pipeline will take a few hours on our 96 CPU machine with an `<offset>` of 78500; the majority of the wait time is from the parts of the code that are not parallelized. If this script fails and you want to try running it again, it might be necessary to remove these directories before trying again: ungoliant_downloads, ungoliant_pipeline_results, ungoliant_filtered, ungoliant_filtered_deduplicated_en.
2. To get a cleaned Wikipedia corups, run `python process_wikipedia_and_push_to_hub.py <date> <huggingface_username>`. Replace the arguments with your desired settings, for example: `20220820 Tristan`. See here for availiable wikipedia snapshot dates: [https://dumps.wikimedia.org/enwiki/](https://dumps.wikimedia.org/enwiki/). This script will take about 30 hours to complete, because it is not parallelized. It requires 100's of GB of RAM or the process will be killed, so we still needed to run it on our big GCP machine. This `push_latest_wikipedia_to_hub.py` is a wrapper around the processing code from this repo: [https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia).


